<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128299627-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-128299627-1');
</script>

  <meta charset="utf-8" />
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/apple-icon.png">
  <link rel="icon" type="image/png" href="/assets/img/favicon.png">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <title>
      BayesHelsinki
    </title>
  <meta content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0, shrink-to-fit=no' name='viewport' />
  <!--     Fonts and icons     -->
  <link href="https://fonts.googleapis.com/css?family=Montserrat:300,700,200" rel="stylesheet" />
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
  <!-- CSS Files -->
  <link href="/assets/css/bootstrap.min.css" rel="stylesheet" />
  <link href="/assets/css/now-ui-kit.css?v=1.3.0" rel="stylesheet" />
  <link href="/assets/demo/demo.css" rel="stylesheet" />
</head>

<body class="profile-page sidebar-collapse">
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg bg-primary fixed-top navbar-transparent " color-on-scroll="400">
    <div class="container">
      <div class="navbar-translate">
          <a class="navbar-brand" href="/index.html" rel="tooltip" title="Blog & Portfolio of Lauri Viljanen" data-placement="bottom">
            BayesHelsinki
          </a>
        <button class="navbar-toggler navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation" aria-controls="navigation-index" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-bar top-bar"></span>
          <span class="navbar-toggler-bar middle-bar"></span>
          <span class="navbar-toggler-bar bottom-bar"></span>
        </button>
      </div>
      <div class="collapse navbar-collapse justify-content-end" id="navigation">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="/about.html">
              <i class="now-ui-icons education_hat"></i>
              <p>About</p>
            </a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog.html">
              <i class="now-ui-icons files_paper"></i>
              <p>Blog</p>
            </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  <!-- End Navbar -->
<div class="wrapper">

    <div class="page-header clear-filter page-header-small" filter-color="black">
      <div class="page-header-image" data-parallax="true" style="background-image:url('./matrix_derivation/bg20.jpg');">
      </div>
      <div class="container">
        <h1 class="title">OLS Derivation in Matrix Form</h1>
      </div>
    </div>

    <div class="section">
      <div class="container">

        <h3 class="title">Introduction</h3>

 <h5 class="description">In the <a href="graphical_derivation.html" >previous post</a>
  we explored the derivation of univariate OLS regression graphically and with some simple algebra.
  To extend the derivation to a multivariate case, we need matrix calculus, but let's first begin with the same SLR model from previous post and show that the matrix form is identical to the one we derived earlier. 
    <br>

    <h3 class="title">Some ground work for matrix notation</h3>

<p>Let's continue with the same data set of artificial weights and heights.</p>

<div class="row justify-content-center">
  <table class="table table-striped table-hover table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
      <thead>
      <tr>
      <th style="text-align:left;font-weight: bold;color: white;background-color: #2c2c2c;">
      name
      </th>
      <th style="text-align:right;font-weight: bold;color: white;background-color: #2c2c2c;">
      weight
      </th>
      <th style="text-align:right;font-weight: bold;color: white;background-color: #2c2c2c;">
      height
      </th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td style="text-align:left;">
      Anna
      </td>
      <td style="text-align:right;">
      65.0
      </td>
      <td style="text-align:right;">
      160.0
      </td>
      </tr>
      <tr>
      <td style="text-align:left;">
      Donald
      </td>
      <td style="text-align:right;">
      75.0
      </td>
      <td style="text-align:right;">
      162.5
      </td>
      </tr>
      <tr>
      <td style="text-align:left;">
      Tom
      </td>
      <td style="text-align:right;">
      70.0
      </td>
      <td style="text-align:right;">
      165.0
      </td>
      </tr>
      <tr>
      <td style="text-align:left;">
      Dick
      </td>
      <td style="text-align:right;">
      75.0
      </td>
      <td style="text-align:right;">
      170.0
      </td>
      </tr>
      <tr>
      <td style="text-align:left;">
      Harry
      </td>
      <td style="text-align:right;">
      82.5
      </td>
      <td style="text-align:right;">
      172.0
      </td>
      </tr>
      </tbody>
      </table>
</div>


<p>To begin with the matrix form derivation, we have to define the relevant matrices for the process.
  Y is a column vector of values that we are trying to predict.
</p>
<p><span class="math display">\[Y=\begin{bmatrix}
    y_1\\
    y_2\\
    ...\\
    ...\\
    y_n\\
    \end{bmatrix}_{n\times 1}=\begin{bmatrix}
  65\\
  75\\
  70\\
  75\\
  82.5\\
  \end{bmatrix}_{5\times 1}
  \]</span></p>
  
<p>The dimensions of the Y vector are <span class="math inline">\(n\times 1\)</span>, 
  which is five rows (<span class="math inline">\(=n\)</span>) by one column in this case. 
</p>

<p>The next matrix is the design matrix X, i.e. the matrix of predictor variables. 
  The columns of the matrix correspond to the independent variables and the rows to observations. 
  If the model includes an intercept, the design matrix has to include a dummy column of ones. Thus the dimensions for the X matrix are
  <span class="math inline">\(n\times k\)</span>, in which k is the number of predictors including the intercept.
</p>
<p><span class="math display">\[X=\begin{bmatrix}
    1 & x_1\\
    1 & x_2\\
    1 & x_3\\
    1 & x_4\\
    1 & x_5\\
    \end{bmatrix}_{5\times 2}=\begin{bmatrix}
  1 & 160\\
  1 & 162.5\\
  1 & 165\\
  1 & 170\\
  1 & 172\\
  \end{bmatrix}_{5\times 2}
  \]</span></p>

<p>Next we need an unknown vector of coefficients. 
  The coefficient vector is column vector with dimensions
  <span class="math inline">\(k\times 1\)</span>. 
  Again k corresponds to the number of predictors including the intercept.</p>

<p><span class="math display">\[\hat{\beta}=
    \begin{bmatrix}
    \hat{\beta}_0\\
    \hat{\beta}_1\\
    \end{bmatrix}_{2\times 1}
    \]</span></p>

<p>Note that the individual scalar coefficients are denoted with a subscript,
  while the vector is denoted simply as <span class="math inline">\(\hat{\beta}\)</span>.</p>

<p>The last component that we need for our model are the residuals (deviations from the predicted values) that our model will make, which is a
    <span class="math inline">\(n\times 1\)</span> vector of residuals.
</p>

<p><span class="math display">\[e=\begin{bmatrix}
    e_1\\
    e_2\\
    e_3\\
    e_4\\
    e_5\\
    \end{bmatrix}_{5\times 1}
    \]</span></p>

<p>And there we have everything. The regression model is:</p>

<p><span class="math display">\[Y=X\hat{\beta}+e\]</span></p>

<p>The above is just short-hand notation for:</p>

<p><span class="math display">\[\begin{bmatrix}
    y_1\\
    y_2\\
    y_3\\
    y_4\\
    y_5\\
    \end{bmatrix}_{5\times 1}
    =
    \begin{bmatrix}
    1 & x_1\\
    1 & x_2\\
    1 & x_3\\
    1 & x_4\\
    1 & x_5\\
    \end{bmatrix}_{5\times 2}
    \times 
    \begin{bmatrix}
    \hat{\beta}_0\\
    \hat{\beta}_1\\
    \end{bmatrix}_{2\times 1}
    +
    \begin{bmatrix}
    e_1\\
    e_2\\
    e_3\\
    e_4\\
    e_5\\
    \end{bmatrix}_{5\times 1}
    \]</span></p>

<p>Note that the beta vector is on left side of the design matrix. This is not a mistake as 
  we want the matrix product of X and beta.
  A matrix product of an <span class="math inline">\(n\times k\)</span> matrix with
  <span class="math inline">\(k\times 1\)</span> vector is possible, 
  where as the other way around would not be possible. 
  If this is new, I suggest you to study basic rules of matrix calculus, 
  but to simplify if you want the matrix product of two matrices, 
  the number of columns in the first matrix has to be equal to the number of rows in the second matrix.
  On the other hand the dimensions of the resulting matrix will have the number of rows from 
  the first matrix and the number of columns from the second. 
  <br>
  Let's see what happens if we multiply X with beta. We know the resulting matrix, has to have dimensions of 
  <span class="math inline">\(n\times 1\ = 5 \times 1\)</span> </p>

  <p><span class="math display">\[
      \begin{bmatrix}
    y_1\\
    y_2\\
    y_3\\
    y_4\\
    y_5\\
    \end{bmatrix}_{5\times 1}
    =
      \begin{bmatrix}
      1 & x_1\\
      1 & x_2\\
      1 & x_3\\
      1 & x_4\\
      1 & x_5\\
      \end{bmatrix}_{5\times 2}
      \times 
      \begin{bmatrix}
      \hat{\beta}_0\\
      \hat{\beta}_1\\
      \end{bmatrix}_{2\times 1}
      +
      \begin{bmatrix}
      e_1\\
      e_2\\
      e_3\\
      e_4\\
      e_5\\
      \end{bmatrix}_{5\times 1}
      \]</span></p>
      <p><span class="math display">\[
          \begin{bmatrix}
          y_1\\
          y_2\\
          y_3\\
          y_4\\
          y_5\\
          \end{bmatrix}_{5\times 1}
          =
      \begin{bmatrix}
      1 * \hat{\beta}_0 + x_1 * \hat{\beta}_1\\
      1 * \hat{\beta}_0 + x_1 * \hat{\beta}_1\\
      1 * \hat{\beta}_0 + x_1 * \hat{\beta}_1\\
      1 * \hat{\beta}_0 + x_1 * \hat{\beta}_1\\
      1 * \hat{\beta}_0 + x_1 * \hat{\beta}_1\\
      \end{bmatrix}_{5\times 1}
      +
      \begin{bmatrix}
      e_1\\
      e_2\\
      e_3\\
      e_4\\
      e_5\\
      \end{bmatrix}_{5\times 1}
      \]</span></p>

      <p><span class="math display">\[
          \begin{bmatrix}
          y_1\\
          y_2\\
          y_3\\
          y_4\\
          y_5\\
          \end{bmatrix}_{5\times 1}
          =
      \begin{bmatrix}
      \hat{\beta}_0 + \hat{\beta}_1x_1 + e_1\\
      \hat{\beta}_0 + \hat{\beta}_1x_1 + e_2\\
      \hat{\beta}_0 + \hat{\beta}_1x_1 + e_3\\
      \hat{\beta}_0 + \hat{\beta}_1x_1 + e_4\\
      \hat{\beta}_0 + \hat{\beta}_1x_1 + e_5\\
      \end{bmatrix}_{5\times 1}
      \]</span></p>
  
<p>From the last step you can see that each value of y is equal to the sum of intercept, slope times x and the residual.
  This is exactly the same as detoning <span class="math inline">\(y_i=\hat{\beta}_0+\hat{\beta}_1x_i+e_i\)</span>.</p>.

  <h3 class="title">Derivation of OLS in matrix form</h3>

<p>We want to derive a vector of unknown coefficients <span class="math inline">\(\hat{\beta}\)</span>
  for the following linear model:
  <span class="math display">\[Y=X\hat{\beta}+e\]</span>
Remember that the objective is exactly the same as last time:
we want to minimize the sum of squared residuals
<span class="math inline">\(min(\Sigma_i^n e^2_i\))</span>.

This time the derivation is just done with matrix algebra and
we want to minimize the matrix product of the residual vector
<span class="math inline">\(e\)</span>. 
The sum of squared residuals is easy to find with vectors by taking the matrix product of the transpose of
residual vector with the original residual vector.
 
<span class="math display">\[
  e'
  \times
  e
  \]</span>

The above is short-hand of saying:

<span class="math display">\[
    \begin{bmatrix}
    e_1 & e_2 & e_3 & e_4 & e_5\\
    \end{bmatrix}_{1\times 5}
    \times
    \begin{bmatrix}
    e_1\\
    e_2\\
    e_3\\
    e_4\\
    e_5\\
    \end{bmatrix}_{5\times 1}
    \]</span>

Again just to illustrate the connection to the scalar derivation, we can see what the above
matrix product would yield us:

<span class="math display">\[
    \begin{bmatrix}
    e_1 * e_1 + e_2 * e_2 + e_3 * e_3 + e_4 * e_4 + e_5 * e_5 \\
    \end{bmatrix}_{1\times 1}
    \]</span>

Note that the result is a <span class="math inline">\(1 \times 1\))</span> vector and thus a scalar
and equal to the minimization problem of the previous post:
  
<span class="math display">\[
    e^2_1 + e^2_2 + e^2_3 + e^2_4 + e^2_5 = \Sigma^{n=5}_i e_i
    \]</span>

Consequently we can minimize:

<span class="math display">\[
    e'
    \times
    e
    \]</span>

and as we note that 

<span class="math display">\[
    e = Y - X \hat{\beta}
    \]</span>

We can denote:

<span class="math display">\[
    e'
    \times
    e
    =
    (Y - X \hat{\beta})'
    (Y - X \hat{\beta})
    \]</span>

Using the properties of <a target="_blank" href="https://en.wikipedia.org/wiki/Transpose">transponations:</a>

<span class="math display">\[
    e'
    \times
    e
    =
    (Y' - \hat{\beta}'X')
    (Y - X \hat{\beta})
    \]</span>

<span class="math display">\[
        =
        Y'Y - Y' X \hat{\beta} - \hat{\beta}'X' Y + \hat{\beta}'X' X \hat{\beta}
        \]</span>

Take heed that <span class="math display">\[
  {Y'}_{1 \times n}
  X_{n \times k}
  \hat{\beta}_{k \times 1}
  = Scalar =
  {\hat{\beta}'}_{1 \times k}
  {X'}_{k \times n}
  Y_{n \times 1}
  \]</span>

The transpose of a scalar is the scalar itself:

<span class="math display">\[
    =
    Y'Y - \hat{\beta}'X' Y - \hat{\beta}'X' Y + \hat{\beta}'X' X \hat{\beta}
    \]</span>

<span class="math display">\[
        =
        Y'Y - 2\hat{\beta}'X' Y + \hat{\beta}'X' X \hat{\beta}
        \]</span>

Remember from previous post that we are trying to solve the minimum of an upward opening parabola and
thus we have to differentiate with the respect to vector beta and find the point where the derivative is zero:

<span class="math display">\[
    \frac{\delta e'e}{\delta \hat{\beta}}=
    - 2X' Y + 2X' X \hat{\beta}
    \]</span>

The above uses the following properties of matrix differentation:

<span class="math display">\[
    \frac{\delta h'd}{\delta h}=d=\frac{\delta hd'}{\delta h}\]</span>
    when h and d are k x 1 vectors. X'Y is a k x 1 vector.
<span class="math display">\[  
    \frac{\delta h'Ah}{\delta h}=2Ah
    \]</span>
when A is a symmetric matrix, which is the case with X'X.
    
<span class="math display">\[
        2X' Y = 2X' X \hat{\beta} \\
        X' X \hat{\beta} = X' Y\\
        \]</span>

Let's take a look on the LHS: <span class="math inline">\({X'}_{k \times n}\)</span>
and
<span class="math inline">\({X}_{n \times k}\))</span>.
The matrix product of such matrix is a square k x k matrix and symmetric.
Next we'll assume that the X'X matrix is <a href="https://en.wikipedia.org/wiki/Invertible_matrix">invertible</a>. 
To simplify, inversion for matrices is the closest thing to a division - an operation not available matrices.
The matrix would not be invertible, for example, in a situtation where two of your predictor variables are perfectly correlated,
i.e. in a case of perfect multicollinearity.

Next we pre-multiply both sides of the equation with the inversion of the matrix 
<span class="math inline">\(
  ({X'}_{k \times n}
  {X}_{n \times k})^{-1}
  \)</span>.

  <span class="math display">\[
      (X'X)^{-1} X' X \hat{\beta} = (X'X)^{-1} X' Y\\
      \]</span>

Now we'll need to remember that multiplying a matrix by its inverse results an identity matrix by definition.
<span class="math inline">\(
    (X'X)^{-1} X'X = I_n
          \)</span>.
          The identity matrix is the following:
          <span class="math display">\[
              I_n=\begin{bmatrix}
              1 & 0 & 0 & ... & 0 \\
              0 & 1 & 0 & ... & 0 \\
              0 & 0 & 1 & ... & 0 \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & 0 & 0 & ... & 1 \\
              \end{bmatrix}
              \]</span>.

Multiplying a matrix with an identity matrix results in the matrix itself:

<span class="math display">\[
    I_n \hat{\beta} = (X'X)^{-1} X' Y\\  
    \hat{\beta} = (X'X)^{-1} X' Y\\
\]</span>

And there you have it. This is the third aspect of the very same derivation. Let's try it out with out data.
The final results appear in the beta vector. The first element of the beta vector is the intercept and the second the slope.

<img class="img-fluid-70" src="./matrix_derivation/matrix_example.jpg">

That's all.
Before proceeding to bayesian regression, we'll cover Maximum Likelihood aspect of OLS regression. This is the topic of the next post.

</p>

      </div>
    </div>
</div>

  <div class="section section-download" id="#download-section" data-background-color="black">    
    <div class="container">
        <div class="section-team text-center">
          <div class="content-center brand">
              <img class="n-logo" src="/assets/img/bh-small.png" alt="">
          </div>
        </div>

        <br>
        <div class="row justify-content-md-center sharing-area text-center">
          <div class="text-center col-md-12 col-lg-8">
            <h3 class = "title">You can reach me through my LinkedIn profile</h3>
          </div>
          <div class="text-center col-md-12 col-lg-8">
            <a target="_blank" href="https://www.linkedin.com/in/lauriviljanen" class="btn btn-neutral btn-icon btn-linkedin btn-lg btn-round" rel="tooltip" title="Lauri Viljanen">
              <i class="fab fa-linkedin"></i>
            </a>
          </div>
        </div>
  </div>
</div>

  <footer class="footer" data-background-color="black">
    <div class=" container ">
      <nav>
        <ul>
          <li>
            <a href="https://www.bayeshelsinki.com">
              BayesHelsinki
            </a>
          </li>
          <li>
            <a href="/about.html">
              About
            </a>
          </li>
          <li>
            <a href="/blog.html">
              Blog
            </a>
          </li>
        </ul>
      </nav>
      <div class="copyright" id="copyright">
        &copy;
        <script>
          document.getElementById('copyright').appendChild(document.createTextNode(new Date().getFullYear()))
        </script>
      </div>
    </div>
  </footer>

  <!--   Core JS Files   -->
  

  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
  <script src="/assets/js/core/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/core/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/core/bootstrap.min.js" type="text/javascript"></script>
  <!--  Plugin for Switches, full documentation here: http://www.jque.re/plugins/version3/bootstrap.switch/ -->
  <script src="/assets/js/plugins/bootstrap-switch.js"></script>
  <!--  Plugin for the Sliders, full documentation here: http://refreshless.com/nouislider/ -->
  <script src="/assets/js/plugins/nouislider.min.js" type="text/javascript"></script>
  <!--  Plugin for the DatePicker, full documentation here: https://github.com/uxsolutions/bootstrap-datepicker -->
  <script src="/assets/js/plugins/bootstrap-datepicker.js" type="text/javascript"></script>
  <!--  Google Maps Plugin    -->
  <script src="https://maps.googleapis.com/maps/api/js?key=YOUR_KEY_HERE"></script>
  <!-- Control Center for Now Ui Kit: parallax effects, scripts for the example pages etc -->
  <script src="/assets/js/now-ui-kit.js?v=1.3.0" type="text/javascript"></script>
  <script>
    $(document).ready(function() {
      nowuiKit.initContactUsMap();
    });
  </script>
</body>

</html>